{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Notebook\n",
    "\n",
    "File to perform experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import gpflow\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models.GaussianProcess import GaussianProcess\n",
    "from acquisition_functions.MES import mes_acq, basic_mes_acq\n",
    "from acquisition_functions.PESMO import pesmo_acq\n",
    "from acquisition_functions.MESMO import mesmo_acq\n",
    "from arguments.arguments import MainArguments\n",
    "\n",
    "from MOObenchmark import MOOackley, MOOcrossit, MOOquadratic\n",
    "from utils.calc_pareto import get_pareto_undominated_by, getSetfromFront\n",
    "\n",
    "from models.MOOEvaluationProblem import MOOEvaluationProblem\n",
    "\n",
    "from pymoo.algorithms.moo.nsga2 import NSGA2\n",
    "from pymoo.factory import get_termination\n",
    "from pymoo.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_function = \"MOOackley/\"\n",
    "acq_function  = \"basic_mes_acq\"\n",
    "exp_id = 89\n",
    "d = 1\n",
    "    \n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "\n",
    "lower_bound = -2\n",
    "upper_bound = 2\n",
    "\n",
    "lowerBounds = [lower_bound]*d\n",
    "upperBounds = [upper_bound]*d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T=1.2\n",
    "\n",
    "def evaluation(x):\n",
    "    return MOOackley(x, c1=-T/2, c2=T/2)\n",
    "\n",
    "N = 1_001\n",
    "X = np.linspace(lower_bound,upper_bound,N)\n",
    "Z = np.zeros((N,2))\n",
    "\n",
    "O = 2\n",
    "C = 0\n",
    "\n",
    "problem = MOOEvaluationProblem(evaluation, lowerBound=lower_bound, upperBound=upper_bound)\n",
    "algorithm = NSGA2()\n",
    "res = minimize( problem, \n",
    "                algorithm,\n",
    "                termination = get_termination(\"n_gen\",100))\n",
    "\n",
    "real_pareto = res.F[np.argsort(res.F[:,0])]\n",
    "\n",
    "for i in range(N):\n",
    "    Z[i]=evaluation(X[i])\n",
    "\n",
    "fig, axs = plt.subplots(1,3, figsize=(15,4))\n",
    "\n",
    "axs[0].plot(X, Z[:,0], 'b')\n",
    "axs[0].plot(X, Z[:,1], 'k')\n",
    "axs[0].plot(res.X, res.F[:,0], 'xr', markersize=5)\n",
    "axs[0].plot(res.X, res.F[:,1], 'xr', markersize=5)\n",
    "\n",
    "axs[1].plot(np.reshape(Z,(-1,2))[:,0], np.reshape(Z,(-1,2))[:,1], 'kx')\n",
    "axs[1].plot(res.F[:,0], res.F[:,1], 'rx')\n",
    "\n",
    "axs[2].plot(res.F[:,0], res.F[:,1], 'x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_acq(GP: GaussianProcess, **kwargs):\n",
    "    while True:\n",
    "        x_rand = np.random.uniform(GP.lowerBounds, GP.upperBounds, GP.d)\n",
    "        if GP.X is None or not x_rand in GP.X:\n",
    "            break\n",
    "    return x_rand, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Previous experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(eval_function+acq_function+\".csv\")\n",
    "df = df[df['exp_id']==exp_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_numpy(str):\n",
    "    str=str[1:-1]\n",
    "    values = []\n",
    "    for v in str.split(\" \"):\n",
    "        if v==\"\":\n",
    "            continue\n",
    "        values.append(float(v))\n",
    "    return np.array(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(df['ns'], df['hp_e']-df['hp_r'], label=\"hp\")\n",
    "plt.plot(df['ns'], df['d_e_r'], label=\"d\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_metrics = None\n",
    "with tf.device('/GPU:1'):\n",
    "    ## Kernerl configuration \n",
    "    k = gpflow.kernels.SquaredExponential()\n",
    "    ### GPs Initialization\n",
    "    GP = GaussianProcess(O, C, d, lowerBounds, upperBounds, k, noise_variance=2e-6)\n",
    "\n",
    "    for n, i in enumerate(df.itertuples()):\n",
    "\n",
    "        x = string_to_numpy(i.x)\n",
    "        y = string_to_numpy(i.y)\n",
    "        print(n, \":  \", x, y)\n",
    "        \n",
    "        ## UPDATE\n",
    "        GP.addSample(x, y)     ## Add new sample to the model\n",
    "        GP.updateGP()                  ## Update data on the GP regressor\n",
    "        GP.optimizeKernel()             ## Optimize kernel hyperparameters\n",
    "        GP.plotSamples()\n",
    "        \n",
    "        ## Evaluate Pareto (distances and hypervolumes)\n",
    "        metrics = GP.evaluatePareto(real_pareto, showparetos = True, saveparetos = False)\n",
    "        metrics['ns'] = len(GP.X)\n",
    "        if df_metrics is None:\n",
    "            df_metrics = pd.DataFrame({k: [v] for k, v in metrics.items()})\n",
    "        else:\n",
    "            df_metrics = pd.concat([df_metrics, pd.DataFrame({k: [v] for k, v in metrics.items()})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_metrics['ns'], abs(df_metrics['hp_e']-df_metrics['hp_r']), label=\"hp\")\n",
    "plt.plot(df_metrics['ns'], df_metrics['d_e_r'], label=\"d\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5ff4c3eb246f2b52b7f298535ee10c6d55e8929659d135abbc88f0b5728f4230"
  },
  "kernelspec": {
   "display_name": "MOO-venv",
   "language": "python",
   "name": "moo-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
